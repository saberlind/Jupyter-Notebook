


from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
reg.coef_








# Code source: Jaques Grobler
# License: BSD 3 clause

import matplotlib.pyplot as plt
import numpy as np

from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Load the diabetes dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)

# Use only one feature
diabetes_X = diabetes_X[:, np.newaxis, 2]

# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes_y[:-20]
diabetes_y_test = diabetes_y[-20:]

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# Make predictions using the testing set
diabetes_y_pred = regr.predict(diabetes_X_test)

# print("y_pred: ", diabetes_y_pred)
# print("y_test: ", diabetes_y_test)
# print("X_test: ", diabetes_X_test)
# The coefficients(系数)
print("Coefficients: \n", regr.coef_)
# The mean squared error (均方误差)
print("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))
# The coefficient of determination: 1 is perfect prediction (决定系数:1为完美预测)
print("Coefficient of determination: %.2f" % r2_score(diabetes_y_test, diabetes_y_pred))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test, color="black")
plt.plot(diabetes_X_test, diabetes_y_pred, color="blue", linewidth=3)

plt.xlabel('x_test')
plt.ylabel('y')
# plt.xticks(())
# plt.yticks(())

plt.show()


import numpy as np

a = np.arange(30).reshape(5,6)
print(a)
b = a[:, np.newaxis, 2]
print(b)
print(b[:-2])
print(b[-2:])





fig, ax = plt.subplots()
ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=".")

low_x, high_x = ax.get_xlim()
low_y, high_y = ax.get_ylim()
low = max(low_x, low_y)
high = min(high_x, high_y)
ax.plot([low, high], [low, high], ls="--", c=".3", alpha=0.5)
ax.set_xlabel("OLS regression coefficients", fontweight="bold")
ax.set_ylabel("NNLS regression coefficients", fontweight="bold")


n_samples, n_features = 200, 50
X = np.random.randn(n_samples, n_features)
# print(X)
true_coef = 3 * np.random.randn(n_features)
# print(true_coef)
# Threshold coefficients to render them non-negative
true_coef[true_coef < 0] = 0
print(true_coef)
y = np.dot(X, true_coef)
# print(y)

y += 5 * np.random.normal(size=(n_samples,))
# np.random.normal(size=(n_samples,))








from sklearn import linear_model
reg = linear_model.Ridge(alpha=.5)
a = reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
print(a)
b = reg.coef_
print(b)
reg.intercept_














import matplotlib.pyplot as plt
import numpy as np

from sklearn import linear_model

# X is the 10x10 Hilbert matrix (希尔伯特矩阵)
X = 1.0 / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
# print(X)
y = np.ones(10)

# Compute Paths
n_alphas = 200
# 生成从-10到-2的等比数列，共n_alphas个数
alphas = np.logspace(-10, -2, n_alphas)

coefs = []
for a in alphas:
    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X, y)
    coefs.append(ridge.coef_)
    # print(ridge.coef_)

# print(coefs)

# Display results
ax = plt.gca()

ax.plot(alphas, coefs)
ax.set_xscale("log")
ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
plt.xlabel("alpha")
plt.ylabel("weights")
plt.title("Ridge coefficients as a function of the regularization")
# plt.axis("tight")
plt.show()


print(np.arange(1, 11))
print(np.arange(0, 10)[:, np.newaxis])
print(np.ones(10))
# np.logspace(-10, -2, n_alphas)
