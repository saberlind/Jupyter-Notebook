{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb46849a-f119-4949-b073-882098cbc4c1",
   "metadata": {},
   "source": [
    "# 1. Ordinary Least Squares\n",
    ">普通最小二乘法\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, w2,......,wn)\n",
    "to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. \n",
    "\n",
    "LinearRegression拟合一个系数w = (w1, w2，......，wn)的线性模型，以最小化数据集中观测目标与线性逼近预测目标之间的残差平方和。\n",
    "\n",
    "\n",
    "LinearRegression will take in its fit method arrays X, y and will store the coefficients w\n",
    " of the linear model in its coef_ member\n",
    "\n",
    " LinearRegression的fit方法接收数组X, y，并存储系数 w 线性模型的coef_成员:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d8d616-b171-496f-9b65-a8e92911fd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938de8d5-bd39-400f-823b-acfdffaf45e3",
   "metadata": {},
   "source": [
    "普通最小二乘法的系数估计依赖于特征的独立性。当特征相关时，设计矩阵的列X\n",
    "具有近似线性依赖性，设计矩阵变得接近奇异，因此，最小二乘估计对观测目标中的随机误差变得非常敏感，产生很大的方差。\n",
    "例如，在没有实验设计的情况下收集数据时，就会出现这种多重共线性的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bed41-3469-4f70-b910-152a269c017d",
   "metadata": {},
   "source": [
    "**Example**\n",
    "* [Linear Regression Example](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FLinear%20Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23d484-f5e8-4fcd-9513-00a8272135ec",
   "metadata": {},
   "source": [
    "## 1.2 Non-Negative Least Squares\n",
    "\n",
    "> 非负最小二乘法\n",
    "\n",
    "It is possible to constrain all the coefficients to be non-negative, which may be useful when they represent some physical or naturally non-negative quantities (e.g., frequency counts or prices of goods). LinearRegression accepts a boolean positive parameter: when set to True Non-Negative Least Squares are then applied.\n",
    "\n",
    "可以将所有系数限制为非负，这在它们表示一些物理上或自然上非负的数量(例如频率计数或商品价格)时可能有用。\n",
    "LinearRegression接受一个布尔值正参数:当设置为True时，应用非负最小二乘\n",
    "\n",
    "**Example**\n",
    "* [Non-Negative Least Squares-Examples](http://localhost:8890/notebooks/SKLearn/examples/Non-Negative%20Least%20Squares.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f5c16-556a-421e-91c9-95b07790aa98",
   "metadata": {},
   "source": [
    "### 1.2.1 Ordinary Least Squares Complexity\n",
    "\n",
    "The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape (n_samples, n_features) this method has a cost of O(n<sub>simples</sub>n<sup>2</sup><sub>features</sub>) \n",
    ",  assuming that n<sub>simples</sub> > n<sub>features</sub>\n",
    "\n",
    "最小二乘法复杂度\n",
    "\n",
    "如果X是一个形状为(n_samples, n_features)的矩阵，则该方法的时间复杂度为 O(n<sub>simples</sub>n<sup>2</sup><sub>features</sub>)。\n",
    "，假设n<sub>simples</sub> > n<sub>features</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203980b5-8f0a-40dd-a80e-8f7c45e203c4",
   "metadata": {},
   "source": [
    "# 2. Ridge regression and classification\n",
    "\n",
    "> Ridge回归与分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b88dce-f938-4199-a78e-ebe7a1d14044",
   "metadata": {},
   "source": [
    "## 2.1 Regression\n",
    "\n",
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares\n",
    "\n",
    "Ridge回归通过对系数的大小施加惩罚，解决了普通最小二乘法的一些问题。Ridge系数最小化了惩罚后的残差平方和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb2a9c0-06b1-44cd-bcbb-353069db9c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5)\n",
      "[0.34545455 0.34545455]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1363636363636364"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "a = reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "print(a)\n",
    "b = reg.coef_\n",
    "print(b)\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57495db4-e660-484c-a3d1-098d494504d4",
   "metadata": {},
   "source": [
    "Note that the class Ridge allows for the user to specify that the solver be automatically chosen by setting solver=\"auto\".  When this option is specified, Ridge will choose between the \"lbfgs\", \"cholesky\", and \"sparse_cg\" solvers.  Ridge will begin checking the conditions shown in the following table from top to bottom.  If the condition is true, the corresponding solver is chosen\n",
    "\n",
    "注意，Ridge类允许用户通过设置solver=\"auto\"来指定自动选择求解器。指定此选项后，Ridge将在\"lbfgs\"、\"cholesky\"和\"sparse_cg\"求解器之间进行选择。Ridge 将开始检查下表所示的条件。如果条件为真，则选择相应的求解器\n",
    "\n",
    "solver参数枚举含义：\n",
    "* 'lbfgs'：The positive=True option is specified.(指定了positive=True选项)\n",
    "* 'cholesky'：The input array X is not sparse.(输入数组X不是稀疏的)\n",
    "* 'sparse_cg'：None of the above conditions are fulfilled.(以上条件都不满足)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ead27-8754-4ea7-bf1c-22bc9ec03222",
   "metadata": {},
   "source": [
    "## 2.2 Classification\n",
    "\n",
    "Ridge回归器有一个分类器变体:RidgeClassifier。该分类器首先将二进制目标转换为{-1,1}，然后将问题视为回归任务，优化与上述相同的目标。预测的类别对应回归器预测的正负号。对于多分类问题，将问题视为多输出回归，预测的类别对应于最高值的输出\n",
    "\n",
    "使用(惩罚的)最小二乘损失来拟合分类模型，而不是更传统的logistic或hinge损失，似乎有问题。然而，在实践中，所有这些模型都可以在准确率或精度/召回率方面得到相似的交叉验证分数，而RidgeClassifier使用的惩罚最小二乘损失允许使用非常不同的数值求解器，具有不同的计算性能配置\n",
    "\n",
    "这种分类器有时也称为具有线性核的最小二乘支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde4cfd-7441-4319-8f26-e64e5f997691",
   "metadata": {},
   "source": [
    "**Example**\n",
    "* [Plot Ridge coefficients as a function of the regularization(绘制Ridge系数作为正则化的函数)](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FPlot%20Ridge%20coefficients%20as%20a%20function%20of%20the%20regularization.ipynb)\n",
    "* [Classification of text documents using sparse features(基于稀疏特征的文本分类)](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FClassification%20of%20text%20documents%20using%20sparse%20features.ipynb)\n",
    "* [Common pitfalls in the interpretation of coefficients of linear models(解释线性模型系数时的常见陷阱)](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FCommon%20pitfalls%20in%20the%20interpretation%20of%20coefficients%20of%20linear%20models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371bd8c-c1bd-4d86-a319-792d37491a30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.1 Ridge Complexity\n",
    "\n",
    "This method has the same order of complexity as [Ordinary Least Squares](http://localhost:8890/notebooks/SKLearn/2.LinearModels.ipynb#1.2.1-Ordinary-Least-Squares-Complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8556362-55f1-4b8b-a429-ed6b2f2308e8",
   "metadata": {},
   "source": [
    "### 2.2.2 Setting the regularization parameter: leave-one-out Cross-Validation\n",
    "\n",
    "RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Leave-One-Out Cross-Validation\n",
    "\n",
    "RidgeCV通过内置的alpha参数交叉验证实现了Ridge回归。该对象的工作方式与GridSearchCV相同，只是它默认使用一个交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ee57bc-f4b1-4e2a-8207-039a153221d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
      "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "print(reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]))\n",
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ec875-1e6c-46ad-87e4-b47806033bda",
   "metadata": {},
   "source": [
    "指定cv属性的值将触发使用GridSearchCV进行交叉验证，例如cv=10表示10-fold cross-validation，而不是Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f091d6-808a-4d70-b671-a32feb3cf64c",
   "metadata": {},
   "source": [
    "# 3 Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704eeea3-a5ef-4df8-bd76-d6a8b9af4998",
   "metadata": {},
   "source": [
    "Lasso是一种估计稀疏系数的线性模型。它在某些情况下是有用的，因为它倾向于选择非零系数较少的解，有效地减少了给定解所依赖的特征的数量。因此，Lasso及其变体是压缩感知领域的基础。在一定条件下，它可以恢复精确的非零系数集(见[压缩感知:基于L1先验的断层成像重建(Lasso)](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py))。\n",
    "\n",
    "在数学上，它由一个附加正则项的线性模型组成。\n",
    "\n",
    "Lasso类的实现使用坐标下降作为拟合系数的算法。参见最小角度回归的另一种实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d9a364-3439-4189-8838-5a2f4c7d8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "print(reg.fit([[0, 0], [1, 1]], [0, 1]))\n",
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e2b42-7d1d-4593-9508-b078040fd5b9",
   "metadata": {},
   "source": [
    "lasso_path函数对于较低级别的任务很有用，因为它计算所有可能值的完整路径上的系数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036a86f-dcfa-4a46-8dd7-9d2ab8651185",
   "metadata": {},
   "source": [
    "**Examples:**\n",
    "\n",
    "* [L1-based models for Sparse Signals](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FL1-based%20models%20for%20Sparse%20Signals.ipynb) 稀疏信号的L1模型\n",
    "* [Compressive sensing: tomography reconstruction with L1 prior (Lasso)](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FCompressive%20sensing%20tomography%20reconstruction%20with%20L1%20prior%20(Lasso).ipynb) 压缩感知:基于L1先验的断层图像重建(Lasso)\n",
    "* [Common pitfalls in the interpretation of coefficients of linear models](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FCommon%20pitfalls%20in%20the%20interpretation%20of%20coefficients%20of%20linear%20models.ipynb) 解释线性模型系数时的常见陷阱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae814c-9845-4c23-9802-ad79656aea24",
   "metadata": {},
   "source": [
    "## 3.1 Setting regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dbadb-b2ca-490e-9c6f-410ef4e112a2",
   "metadata": {},
   "source": [
    "The alpha parameter controls the degree of sparsity of the estimated coefficients.\n",
    "\n",
    "alpha参数控制估计系数的稀疏程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de2145-5f63-4652-8032-1eba15e5bccd",
   "metadata": {},
   "source": [
    "**Using cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5e2d1-85cc-4c0a-85e9-ab2d41658abf",
   "metadata": {},
   "source": [
    "scikit-learn公开了通过交叉验证设置Lasso alpha参数的对象:LassoCV和LassoLarsCV。LassoLarsCV基于最小角度回归算法，如下所述。\n",
    "\n",
    "对于具有许多共线特征的高维数据集，LassoCV通常是最可取的。然而，LassoLarsCV具有探索alpha参数更多相关值的优势，并且如果样本数量与特征数量相比非常小，它通常比LassoCV更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e271e-3546-4ba4-aeb0-4aa31f5e1bfe",
   "metadata": {},
   "source": [
    "**Information-criteria based model selection**\n",
    "\n",
    "> 基于信息标准的模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a777d-f649-40c9-9518-205ade4620f2",
   "metadata": {},
   "source": [
    "另外，估计器LassoLarsIC建议使用Akaike信息准则(AIC)和Bayes(贝叶斯)信息准则(BIC)。这是一种计算成本更低的方法来找到alpha的最佳值，因为在使用k-fold交叉验证时，正则化路径只计算一次，而不是k+1次。\n",
    "\n",
    "实际上，这些标准是在样本内训练集上计算的。简而言之，它们通过灵活性惩罚了不同Lasso模型过于乐观的分数(参见下面的“数学细节”部分)。\n",
    "\n",
    "然而，这种准则需要对解的自由度进行适当的估计，是针对大样本(渐近结果)推导出来的，并假设正确的模型是调查中的候选。当问题条件不好时(例如特征比样本多)，它们也倾向于崩溃。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f452ef-ea4a-42c6-b6d5-3dff6af6dc39",
   "metadata": {},
   "source": [
    "**Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da6b1c-80e8-46bc-9ad3-cebfa508c564",
   "metadata": {},
   "source": [
    "* [Lasso model selection: AIC-BIC / cross-validation](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FLasso%20model%20selection%20%20AIC-BIC%20%20cross-validation.ipynb) Lasso模型选择:AIC-BIC /交叉验证\n",
    "* [Lasso model selection via information criteria](http://localhost:8890/notebooks/SKLearn/examples/Lasso%20model%20selection%20via%20information%20criteria.ipynb) 基于信息准则的Lasso模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec255a3b-fd6c-42de-a828-acde285fa80c",
   "metadata": {},
   "source": [
    "**AIC and BIC criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc7866-1b8b-4904-9495-b2fb75ff0a0f",
   "metadata": {},
   "source": [
    "The definition of AIC (and thus BIC) might differ in the literature. In this section, we give more information regarding the criterion computed in scikit-learn.\n",
    "\n",
    "AIC(以及BIC)的定义在文献中可能不同。在本节中，我们将提供有关scikit-learn中计算的准则的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879864d6-2173-4b3d-9e7c-94affef1bf8a",
   "metadata": {},
   "source": [
    "**Comparison with the regularization parameter of SVM**\n",
    "\n",
    "**与SVM的正则化参数进行比较**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ddd6a-8f45-417e-8243-042681245bf7",
   "metadata": {},
   "source": [
    "alpha和SVM的正则化参数C之间的等价性由alpha = 1 / C或alpha = 1 / (n_samples * C)给出，这取决于估计器和模型优化的精确目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59b1ad-52ab-4a29-a95f-038991f91583",
   "metadata": {},
   "source": [
    "# 4. Multi-task Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f76913-af89-4a8a-a180-660ac04f5fbd",
   "metadata": {},
   "source": [
    "MultiTaskLasso是一个线性模型，可以联合估计多元回归问题的稀疏系数:y是一个二维数组，形状为(n_samples, n_tasks)。约束条件是选择的特征对于所有回归问题(也称为任务)都是相同的。\n",
    "\n",
    "下图比较了使用简单Lasso和MultiTaskLasso得到的系数矩阵W中非零元素的位置。Lasso估计产生分散的非零值，而MultiTaskLasso的非零值是完整的列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d096b-f2c1-4b5b-904a-2ad8374581a8",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "* [Joint feature selection with multi-task Lasso](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FJoint%20feature%20selection%20with%20multi-task%20Lasso.ipynb)  基于多任务Lasso的联合特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359524b-a84d-454e-84a2-6d64b839452a",
   "metadata": {},
   "source": [
    "# 5. Elastic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcd2e0-ce0d-4e49-b309-a4ef384cb039",
   "metadata": {},
   "source": [
    "ElasticNet是一个线性回归模型，使用L1和l2范数正则化系数进行训练。这种组合允许学习一个稀疏模型，其中很少有权重非零，如Lasso，同时仍然保持Ridge的正则化属性。我们使用l1_ratio参数控制L1和L2的凸组合。\n",
    "\n",
    "当有多个特征相互关联时，Elastic-net很有用。Lasso可能随机选择其中一种，而elastic-net可能同时选择两种。\n",
    "\n",
    "在Lasso和Ridge之间进行权衡的一个实际优势是，它允许Elastic-Net在旋转时继承Ridge的一些稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b53b6-5624-4a36-861d-34d7721657bd",
   "metadata": {},
   "source": [
    "The class ElasticNetCV can be used to set the parameters alpha (α) and l1_ratio (β) by cross-validation.\n",
    "\n",
    "ElasticNetCV类可以通过交叉验证来设置参数alpha (α)和l1_ratio (β)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fadef7-09ba-452c-b7a8-d22ab1b9ac8d",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "* [L1-based models for Sparse Signals](http://localhost:8890/notebooks/SKLearn/examples/L1-based%20models%20for%20Sparse%20Signals.ipynb) 稀疏信号的L1模型\n",
    "* [Lasso and Elastic Net](http://localhost:8890/notebooks/SKLearn/examples/Lasso%20and%20Elastic%20Net.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830c081-83f7-4ed7-8c3d-f2fa3bbbee15",
   "metadata": {},
   "source": [
    "# 6. Multi-task Elastic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4e3c2-1cb1-4950-bf6c-fb4518d17716",
   "metadata": {},
   "source": [
    "MultiTaskElasticNet是一个弹性网络模型，联合估计多元回归问题的稀疏系数:Y是形状的二维数组(n_samples, n_tasks)。约束条件是选择的特征对于所有回归问题(也称为任务)都是相同的。\n",
    "\n",
    "在MultiTaskElasticNet类中的实现使用坐标下降作为算法来拟合系数。\n",
    "\n",
    "MultiTaskElasticNetCV类可以通过交叉验证来设置参数alpha (α)和l1_ratio (β)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7ef0b-49a7-4e3b-9389-859810105184",
   "metadata": {},
   "source": [
    "# 7. Least Angle Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323497e-8611-4028-a744-d8f6717867e6",
   "metadata": {},
   "source": [
    "最小角度回归(Least-angle regression, LARS)是一种用于高维数据的回归算法，由Bradley Efron、Trevor Hastie、Iain Johnstone和Robert Tibshirani开发。LARS类似于逐步前向回归。在每一步，它都会找到与目标最相关的特征。当有多个特征具有相等的相关性时，它不是沿着同一个特征继续前进，而是在特征之间等角的方向前进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ea51d-01f8-4ca8-9619-2a8c92255c42",
   "metadata": {},
   "source": [
    "The advantages of LARS are:(LARS的优点)\n",
    "* 在特征数量显著大于样本数量的情况下，它在数值上是有效的。\n",
    "* 它的计算速度与前向选择一样快，并具有与普通最小二乘法相同的复杂度。\n",
    "* 它产生一个完整的分段线性解决路径，这在交叉验证或类似的调整模型的尝试中很有用。\n",
    "* 如果两个特征与目标几乎相等，那么它们的系数应该以大致相同的速率增加。因此，该算法的行为与直觉预期一致，也更稳定。\n",
    "* 它很容易修改为其他估计器(如Lasso)提供解决方案。\n",
    "\n",
    "The disadvantages of the LARS method include:(LARS方法的缺点包括)\n",
    "* 由于LARS是基于残差的迭代拟合，因此它似乎对噪声的影响特别敏感。Weisberg在Efron et al. (2004) Annals of Statistics一文的讨论部分详细讨论了这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0fbce-b26f-4551-99da-e339ee483f20",
   "metadata": {},
   "source": [
    "The LARS model can be used via the estimator Lars, or its low-level implementation lars_path or lars_path_gram\n",
    "\n",
    "LARS模型可以通过估计器LARS或其底层实现lars_path或lars_path_gram来使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785934e9-faed-473a-aac1-d352cf1c052c",
   "metadata": {},
   "source": [
    "# 8. LARS Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59279aeb-ad94-4157-9c20-6ada1530639c",
   "metadata": {},
   "source": [
    "LassoLars是一个使用LARS算法实现的lasso模型，与基于坐标下降的实现不同，它产生的精确解是其系数范数的分段线性函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe2b09f-97c9-4ffe-be78-adce3b62ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLars(alpha=0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6, 0. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "print(reg.fit([[0, 0], [1, 1]], [0, 1]))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3e2b9-e5ca-4057-a9d5-3fdfc6e4a388",
   "metadata": {},
   "source": [
    "**Example**\n",
    "* [Lasso path using LARS](http://localhost:8890/notebooks/SKLearn%2Fexamples%2FLasso%20path%20using%20LARS.ipynb) 使用LARS的Lasso路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef71d2-dc52-418b-ae66-b90cc7b680be",
   "metadata": {},
   "source": [
    "Lars算法几乎是免费地提供了正则化参数中系数的完整路径，因此常见的操作是使用lars_path或lars_path_gram函数之一来检索该路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae827f38-eb14-40c3-9f39-ec5cd44b207c",
   "metadata": {},
   "source": [
    "# 9. Orthogonal Matching Pursuit (OMP)\n",
    "\n",
    "正交匹配追踪(OMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ac008-0a32-4d5f-9691-5025a7407e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
